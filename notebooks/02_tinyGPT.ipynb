{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Tiny Transformer (Mini-GPT)\nSelf-attention, MHA, FFN, residuals, layernorm.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Setup\n!pip -q install torch --upgrade\nimport math, random, textwrap, requests\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom tqdm.auto import tqdm\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"; torch.manual_seed(1337); print(\"Device:\", device)\n\n# Data\ntext = requests.get(\"https://www.gutenberg.org/files/11/11-0.txt\").text\ntext = text[:500_000]\nchars = sorted(list(set(text))); vocab_size = len(chars)\nstoi = {ch:i for i,ch in enumerate(chars)}; itos = {i:ch for ch,i in stoi.items()}\nencode = lambda s: torch.tensor([stoi[c] for c in s], dtype=torch.long)\ndecode = lambda t: \"\".join(itos[int(i)] for i in t)\ndata = torch.tensor([stoi[c] for c in text], dtype=torch.long)\nn = int(0.9*len(data)); train_data, val_data = data[:n], data[n:]\n\nbatch_size, block_size = 64, 128\nn_embd, n_head, n_layer, dropout = 192, 6, 4, 0.1\nmax_steps, eval_interval, lr = 2500, 250, 3e-4\n\ndef get_batch(split):\n    src = train_data if split==\"train\" else val_data\n    ix = torch.randint(len(src)-block_size-1, (batch_size,))\n    x = torch.stack([src[i:i+block_size] for i in ix])\n    y = torch.stack([src[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\nclass Head(nn.Module):\n    def __init__(self, n_embd, head_size, block_size, dropout):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x); q = self.query(x)\n        wei = q @ k.transpose(-2,-1) / math.sqrt(k.size(-1))\n        wei = wei.masked_fill(self.tril[:T,:T]==0, float(\"-inf\"))\n        wei = F.softmax(wei, dim=-1); wei = self.dropout(wei)\n        v = self.value(x); out = wei @ v\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_embd, n_head, block_size, dropout):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.heads = nn.ModuleList([Head(n_embd, head_size, block_size, dropout) for _ in range(n_head)])\n        self.proj = nn.Linear(n_embd, n_embd); self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out)); return out\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd, dropout):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd), nn.GELU(), nn.Linear(4*n_embd, n_embd), nn.Dropout(dropout))\n    def forward(self, x): return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size, dropout):\n        super().__init__()\n        self.sa = MultiHeadAttention(n_embd, n_head, block_size, dropout)\n        self.ff = FeedForward(n_embd, dropout)\n        self.ln1 = nn.LayerNorm(n_embd); self.ln2 = nn.LayerNorm(n_embd)\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x)); x = x + self.ff(self.ln2(x)); return x\n\nclass TinyGPT(nn.Module):\n    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout):\n        super().__init__()\n        self.token_emb = nn.Embedding(vocab_size, n_embd)\n        self.pos_emb   = nn.Embedding(block_size, n_embd)\n        self.blocks    = nn.ModuleList([Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n        self.ln_final  = nn.LayerNorm(n_embd)\n        self.lm_head   = nn.Linear(n_embd, vocab_size)\n        self.block_size= block_size\n    def forward(self, idx, targets=None):\n        B,T = idx.shape\n        tok = self.token_emb(idx)\n        pos = self.pos_emb(torch.arange(T, device=idx.device))\n        x = tok + pos\n        for blk in self.blocks: x = blk(x)\n        x = self.ln_final(x)\n        logits = self.lm_head(x)\n        loss=None\n        if targets is not None: loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens=300, temperature=1.0, top_k=None):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -self.block_size:]\n            logits,_ = self(idx_cond)\n            logits = logits[:,-1,:]/temperature\n            if top_k is not None:\n                v,_ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:,[-1]]] = -float(\"inf\")\n            probs = F.softmax(logits, dim=-1)\n            nxt = torch.multinomial(probs, 1)\n            idx = torch.cat([idx, nxt], dim=1)\n        return idx\n\nmodel = TinyGPT(vocab_size, n_embd, n_head, n_layer, block_size, dropout).to(device)\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\n\n@torch.no_grad()\ndef estimate_loss():\n    out={}; model.eval()\n    for split in [\"train\",\"val\"]:\n        losses=[]; \n        for _ in range(20):\n            xb,yb = get_batch(split); _,loss = model(xb,yb); losses.append(loss.item())\n        out[split]=sum(losses)/len(losses)\n    model.train(); return out\n\nfor step in range(max_steps):\n    if step % eval_interval == 0:\n        l = estimate_loss(); print(f\"step {step:4d}: train {l['train']:.3f} | val {l['val']:.3f}\")\n    xb,yb = get_batch(\"train\"); _,loss = model(xb,yb)\n    opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n\nprint(\"Training done.\")\n\ndef sample_text(prompt=\"Alice\", max_new_tokens=400, temperature=1.0, top_k=None):\n    start = encode(prompt).unsqueeze(0).to(device)\n    out = model.generate(start, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)[0].tolist()\n    return decode(out)\n\nprint(\"\\n=== SAMPLE (temp=1.0) ===\\n\"); print(textwrap.fill(sample_text(\"Alice\", 400, 1.0, None), width=90))\nprint(\"\\n=== SAMPLE (temp=0.8, top_k=50) ===\\n\"); print(textwrap.fill(sample_text(\"Alice\", 400, 0.8, 50), width=90))\n\n# Save checkpoint (optional)\nimport torch\nckpt = \"/content/tinygpt_char_alice.pt\"\ntorch.save({\"model\": model.state_dict()}, ckpt); print(\"Saved:\", ckpt)\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}