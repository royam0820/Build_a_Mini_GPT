{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Character-Level Language Model (Bigram)\nFast first win on *Alice in Wonderland*.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Setup\n!pip -q install torch --upgrade\nimport math, random, textwrap, requests\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom tqdm.auto import tqdm\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"; torch.manual_seed(1337); print(\"Device:\", device)\n\n# Data\ntext = requests.get(\"https://www.gutenberg.org/files/11/11-0.txt\").text\ntext = text[:300_000]\nchars = sorted(list(set(text))); vocab_size = len(chars)\nstoi = {ch:i for i,ch in enumerate(chars)}; itos = {i:ch for ch,i in stoi.items()}\nencode = lambda s: torch.tensor([stoi[c] for c in s], dtype=torch.long)\ndecode = lambda t: \"\".join(itos[int(i)] for i in t)\n\ndata = torch.tensor([stoi[c] for c in text], dtype=torch.long)\nn = int(0.9*len(data)); train_data, val_data = data[:n], data[n:]\nbatch_size, block_size = 64, 1\ndef get_batch(split):\n    src = train_data if split==\"train\" else val_data\n    ix = torch.randint(len(src)-block_size-1, (batch_size,))\n    x = torch.stack([src[i:i+block_size] for i in ix])\n    y = torch.stack([src[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\nclass BigramLM(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__(); self.token_emb = nn.Embedding(vocab_size, vocab_size)\n    def forward(self, idx, targets=None):\n        logits = self.token_emb(idx); loss=None\n        if targets is not None: loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens=200, temperature=1.0):\n        for _ in range(max_new_tokens):\n            logits,_ = self(idx); logits = logits[:,-1,:]/temperature\n            probs = F.softmax(logits, dim=-1); nxt = torch.multinomial(probs, 1)\n            idx = torch.cat([idx, nxt], dim=1)\n        return idx\n\nmodel = BigramLM(vocab_size).to(device)\nopt = torch.optim.AdamW(model.parameters(), lr=1e-2)\nmax_steps, eval_interval = 2000, 200\n\n@torch.no_grad()\ndef estimate_loss():\n    out={}; model.eval()\n    for split in [\"train\",\"val\"]:\n        losses=[]; \n        for _ in range(20):\n            xb,yb = get_batch(split); _,loss = model(xb,yb); losses.append(loss.item())\n        out[split]=sum(losses)/len(losses)\n    model.train(); return out\n\nfor step in range(max_steps):\n    if step % eval_interval == 0:\n        l = estimate_loss(); print(f\"step {step:4d}: train {l['train']:.3f} | val {l['val']:.3f}\")\n    xb,yb = get_batch(\"train\"); _,loss = model(xb,yb)\n    opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n\nprint(\"Training done.\")\nfor temp in [0.7,1.0,1.3]:\n    ctx = torch.tensor([[torch.randint(vocab_size, (1,)).item()]], device=device)\n    out = model.generate(ctx, 300, temperature=temp)[0].tolist()\n    print(f\"\\n=== SAMPLE (temp={temp}) ===\\n\"); print(textwrap.fill(decode(out), width=90))\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}