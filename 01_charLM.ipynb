{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/Build_a_Mini_GPT/blob/main/01_charLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkL_fv8tRESn"
      },
      "source": [
        "# Character-Level Language Model (Bigram)\n",
        "Fast first win on *Alice in Wonderland*.\n"
      ],
      "id": "AkL_fv8tRESn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccc7fe9a"
      },
      "source": [
        "# Uninstall conflicting libraries\n",
        "!pip uninstall -y fastai torchaudio torchvision\n",
        "\n",
        "# Reinstall torch and compatible versions of fastai, torchaudio, and torchvision\n",
        "!pip install torch==2.9.0 fastai torchaudio torchvision --upgrade"
      ],
      "id": "ccc7fe9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "!pip -q install torch --upgrade\n",
        "import math, random, textwrap, requests\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from tqdm.auto import tqdm # progress bar\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"; torch.manual_seed(1337); print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTI0RUuck0Bl",
        "outputId": "ebe8a497-ed41-44ad-ecc2-d1fcb4614004"
      },
      "id": "aTI0RUuck0Bl",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: `torch.manual_seed(1337)`Sets the random seed for PyTorch to 1337. This\n",
        "ensures that the random number generation is reproducible, which is important for debugging and comparing results."
      ],
      "metadata": {
        "id": "4YGSRy_6lqXH"
      },
      "id": "4YGSRy_6lqXH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset : data loading, preprocessing, and preparation for training the language model.\n",
        "\n",
        "# Fetching the text of \"Alice's Adventures in Wonderland\" from Project Gutenberg\n",
        "text = requests.get(\"https://www.gutenberg.org/files/11/11-0.txt\").text\n",
        "text = text[:300_000]\n",
        "chars = sorted(list(set(text))); vocab_size = len(chars)\n",
        "\n",
        "# dictionaries:  string to integer (stoi) and integer to string (itos)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}; itos = {i:ch for ch,i in stoi.items()}\n",
        "\n",
        "# encoding string to tensor\n",
        "encode = lambda s: torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
        "# decoding tensor to string\n",
        "decode = lambda t: \"\".join(itos[int(i)] for i in t)\n",
        "\n",
        "# convert the entire text inot a Pytorch tensor\n",
        "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
        "# splitting the dataset - 90% is used for training\n",
        "n = int(0.9*len(data)); train_data, val_data = data[:n], data[n:]\n",
        "batch_size, block_size = 64, 1\n",
        "\n",
        "def get_batch(split):\n",
        "    src = train_data if split==\"train\" else val_data\n",
        "    ix = torch.randint(len(src)-block_size-1, (batch_size,))\n",
        "    x = torch.stack([src[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([src[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "Cw4tScQWk2Ji"
      },
      "id": "Cw4tScQWk2Ji",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: `batch_size, block_size = 64, 1`\n",
        "These lines define the batch_size (number of independent sequences processed in parallel) and block_size (the maximum context length for predictions). In this bigram model, block_size is 1, meaning the model only considers the previous character to predict the next one.\n",
        "\n",
        "`def get_batch(split)`: ...: This function generates a batch of data for training or validation. It randomly selects batch_size starting indices from the specified data split (train_data or val_data) and creates input x and target y tensors. For a bigram model, x contains a single character and y contains the next character."
      ],
      "metadata": {
        "id": "VIhfJpEDp9sP"
      },
      "id": "VIhfJpEDp9sP"
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset split (training and evaluation)\n",
        "len(train_data), len(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbB_rBCyohLo",
        "outputId": "cc97c3a2-c17c-4a57-eccf-52056fb9a905"
      },
      "id": "UbB_rBCyohLo",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(130226, 14470)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bj1vFtQsq4u1"
      },
      "id": "bj1vFtQsq4u1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The standard way to define neural networks model in Pytorch."
      ],
      "metadata": {
        "id": "Cmo7Juoqq5eP"
      },
      "id": "Cmo7Juoqq5eP"
    },
    {
      "cell_type": "code",
      "source": [
        "# The Bigram Language Model\n",
        "class BigramLM(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__(); self.token_emb = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_emb(idx); loss=None\n",
        "        if targets is not None: loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=200, temperature=1.0):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits,_ = self(idx); logits = logits[:,-1,:]/temperature\n",
        "            probs = F.softmax(logits, dim=-1); nxt = torch.multinomial(probs, 1)\n",
        "            idx = torch.cat([idx, nxt], dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "bKqNP1QHk5hJ"
      },
      "id": "bKqNP1QHk5hJ",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: `super().__init__()`: This line calls the constructor of the parent class (nn.Module), which is necessary for proper initialization.\n",
        "\n",
        "`self.token_emb = nn.Embedding(vocab_size, vocab_size)`: This line creates an embedding layer. An embedding layer is essentially a lookup table. In this case, it takes an integer representing a character and returns a vector of size vocab_size. This vector can be interpreted as the \"embedding\" or representation of that character. For a simple bigram model, the embedding dimension being equal to the vocabulary size means that the embedding for each character is a one-hot encoded vector, but with trainable weights.\n",
        "\n",
        "`def forward(self, idx, targets=None)`: This is the forward pass method of the model. It defines how the input data flows through the model.\n",
        "- `idx`: This is the input tensor, representing the sequence of characters (as integer indices). For a bigram model, each element in the sequence is a single character.\n",
        "- `targets=None`: This is an optional argument for the target characters (the characters the model is trying to predict). It's used during training to calculate the loss.\n",
        "- `logits` = self.token_emb(idx): This line passes the input indices idx through the embedding layer (self.token_emb). It retrieves the embedding vector for each input character. The output `logits` will have a shape of (`batch_size`, `block_size`, `vocab_size`).\n",
        "- loss=None: Initializes the loss variable to None.\n",
        "\n",
        "`@torch.no_grad()`: This decorator indicates that the following method (generate) should not track gradients. This is important for inference (generating new text) because we don't need to compute gradients for backpropagation during this phase.\n",
        "\n",
        "`def generate(self, idx, max_new_tokens=200, temperature=1.0)`: This method generates new text based on an initial input sequence.\n",
        "idx: The initial input tensor (the context) to start generating from.\n",
        "- `max_new_tokens=200`: The maximum number of new characters to generate.\n",
        "- `temperature=1.0`: Controls the randomness of the generated text. Higher temperatures lead to more random outputs, while lower temperatures make the output more deterministic.\n",
        "- `for _ in range(max_new_tokens)`: This loop runs for the specified number of new tokens to generate.\n",
        "- `logits,_ = self(idx)`: Performs a forward pass of the model using the current context idx. It gets the logits (predictions) for the next character. We only need the logits here, so we ignore the loss.\n",
        "- `logits = logits[:,-1,:]/temperature`: Takes the logits for the last character in the sequence (the one we want to predict) and applies the temperature. Dividing by temperature scales the logits, making the probability distribution sharper (lower temperature) or smoother (higher temperature).\n",
        "- `probs = F.softmax(logits, dim=-1)`: Converts the logits into probability distribution over the vocabulary using the softmax function.\n",
        "`nxt = torch.multinomial(probs, 1)`: Samples the next character from the probability distribution using multinomial sampling. This introduces randomness based on the probabilities.\n",
        "- `idx = torch.cat([idx, nxt], dim=1)`: Appends the newly generated character (`nxt`) to the input sequence idx, which becomes the new context for the next step of generation.\n",
        "- `return idx`: After generating max_new_tokens, the method returns the complete generated sequence (including the initial context).\n",
        "\n",
        "In summary, this BigramLM class defines a simple neural network model that learns to predict the next character based on the current character. The forward method is used for training and calculating the loss, while the generate method is used for generating new text by iteratively predicting and appending the next character.\n",
        "\n"
      ],
      "metadata": {
        "id": "_johTrpqrInK"
      },
      "id": "_johTrpqrInK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "# model instantiation\n",
        "model = BigramLM(vocab_size).to(device)\n",
        "\n",
        "# the optimization algorithm\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
        "\n",
        "# number of training steps\n",
        "max_steps, eval_interval = 2000, 200\n",
        "\n",
        "# estimating the loss\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out={}; model.eval()\n",
        "    for split in [\"train\",\"val\"]:\n",
        "        losses=[];\n",
        "        for _ in range(20):\n",
        "            xb,yb = get_batch(split); _,loss = model(xb,yb); losses.append(loss.item())\n",
        "        out[split]=sum(losses)/len(losses)\n",
        "    model.train(); return out\n",
        "\n",
        "for step in range(max_steps):\n",
        "    if step % eval_interval == 0:\n",
        "        l = estimate_loss(); print(f\"step {step:4d}: train {l['train']:.3f} | val {l['val']:.3f}\")\n",
        "    xb,yb = get_batch(\"train\"); _,loss = model(xb,yb)\n",
        "    opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
        "\n",
        "print(\"Training done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4rQoAAelAXJ",
        "outputId": "e62cc401-58f5-46ab-b98a-4449ad5bc9fe"
      },
      "id": "Y4rQoAAelAXJ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    0: train 4.721 | val 4.750\n",
            "step  200: train 3.314 | val 3.354\n",
            "step  400: train 2.768 | val 2.802\n",
            "step  600: train 2.575 | val 2.539\n",
            "step  800: train 2.620 | val 2.640\n",
            "step 1000: train 2.429 | val 2.565\n",
            "step 1200: train 2.473 | val 2.554\n",
            "step 1400: train 2.451 | val 2.471\n",
            "step 1600: train 2.487 | val 2.458\n",
            "step 1800: train 2.426 | val 2.520\n",
            "Training done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can fine-tune this model by adjusting the hyperparameters in the training loop. The most common parameters to experiment with are:\n",
        "- **Learning Rate (lr)**: Controls the step size of the optimizer. A smaller learning rate might lead to finer tuning, while a larger one can speed up training but might overshoot the optimal parameters.\n",
        "- **Number of Training Steps (max_steps)**: Determines how long the model trains. More steps can lead to better performance but also take more time.\n",
        "- **Batch Size (batch_size)**: The number of samples processed in each training iteration. Larger batch sizes can provide a more accurate estimate of the gradient but require more memory.\n",
        "- **Evaluation Interval (eval_interval)**: How often the model is evaluated during training. More frequent evaluation gives better insight into the training progress but adds overhead."
      ],
      "metadata": {
        "id": "ZQfmMCM_uXNf"
      },
      "id": "ZQfmMCM_uXNf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "for temp in [0.7,1.0,1.3]:\n",
        "    # create an initial context tensor (ctx) for text generation\n",
        "    ctx = torch.tensor([[torch.randint(vocab_size, (1,)).item()]], device=device)\n",
        "    # generate new text\n",
        "    out = model.generate(ctx, 300, temperature=temp)[0].tolist()\n",
        "    print(f\"\\n=== SAMPLE (temp={temp}) ===\\n\"); print(textwrap.fill(decode(out), width=90))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htZN_VivlGNC",
        "outputId": "8c07fbec-e119-4c74-f7e4-417745ab2b3d"
      },
      "id": "htZN_VivlGNC",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SAMPLE (temp=0.7) ===\n",
            "\n",
            "—oraisant y, “I re, as w it wou  waner that t it Qut oule he be t ck Whico te prlery t\n",
            "owan howour, _ are t t wasainowousut’ly “Whinoulofa the t se her isthes, _bed s_ ngithis\n",
            "asalery llyE touseseld ice sthintre her g ge sthing. gingh hin hat sonke id ng, rote\n",
            "inghed t t athe s bor tlf tht rs shoonga\n",
            "\n",
            "=== SAMPLE (temp=1.0) ===\n",
            "\n",
            "3ghe win’s t wiAldd th ‘ù1xitheshaws win, shile shay bushand prok sshioimpat a waim the t\n",
            "wariore t agNSusor, wa Nom: ly Qut. maitwsth!” artZ)  t  mine  the’llishitopoce hof tte\n",
            "izzFims onel) t tas! stf in tin,f, hemphowiele I t htho n it satht\n",
            "h-tyishentlleshougsousthorerusted an oithypot she.—” hit\n",
            "\n",
            "=== SAMPLE (temp=1.3) ===\n",
            "\n",
            "e-Fa n’tthand-)‘Ww g. o tCd nkùHogel deeneynclute‘_; whidg—” aid:]Allw‘Jtheen Dof\n",
            "wquthe.nerve pha pats! abPugofJ” tondou Alù3*[NDifralled athelo iùD, shele.” ouceshelen\n",
            "“br ac-Ca “rr h f s bea “She ile,” lleLaggo id sid t,” D” y Grn, chas. s f s—lo )\n",
            "iEdiniliA whothan oosuled, d—ERIois ideno _ athid\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}